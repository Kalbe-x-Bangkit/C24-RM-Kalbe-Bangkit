{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def load_image(image_name, image_folder):\n",
    "    image_path = os.path.join(image_folder, image_name)\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            return img.copy()  # Copy the image object to avoid closing\n",
    "    except Exception as e:\n",
    "        return None, str(e)\n",
    "\n",
    "def create_dataset(csv_file, image_folder, batch_size=1000, max_workers=4):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    total_rows = len(df)\n",
    "    dataset = []\n",
    "    error_log = []\n",
    "\n",
    "    for start_idx in range(0, total_rows, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total_rows)\n",
    "        batch_df = df[start_idx:end_idx]\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_row = {executor.submit(load_image, row['name'], image_folder): row for _, row in batch_df.iterrows()}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_row), total=len(future_to_row), desc=f\"Processing batch {start_idx // batch_size + 1}\"):\n",
    "                row = future_to_row[future]\n",
    "                try:\n",
    "                    image = future.result()\n",
    "                    if isinstance(image, tuple):  # Checking if an error occurred\n",
    "                        error_log.append((row['name'], image[1]))\n",
    "                    else:\n",
    "                        dataset.append({\n",
    "                            \"image\": image,\n",
    "                            \"image_id\": row['id'],\n",
    "                            \"caption\": row['caption']\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    error_log.append((row['name'], str(e)))\n",
    "\n",
    "    # Save error log to a file\n",
    "    with open('error_log.txt', 'w') as f:\n",
    "        for error in error_log:\n",
    "            f.write(f\"{error[0]}: {error[1]}\\n\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "csv_file = 'train/radiologytraindata.csv'\n",
    "image_folder = 'train/radiology/images'\n",
    "train_dataset = create_dataset(csv_file, image_folder)\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'test/radiologytraindata.csv'\n",
    "image_folder = 'test/radiology/images'\n",
    "test_dataset = create_dataset(csv_file, image_folder)\n",
    "\n",
    "# Example usage\n",
    "csv_file = 'validation/radiologytraindata.csv'\n",
    "image_folder = 'validation/radiology/images'\n",
    "val_dataset = create_dataset(csv_file, image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "def convert_to_datasetdict(dataset_list):\n",
    "    # Convert list of dictionaries to a format suitable for Dataset\n",
    "    images = [data['image'] for data in dataset_list]\n",
    "    image_ids = [data['image_id'] for data in dataset_list]\n",
    "    captions = [data['caption'] for data in dataset_list]\n",
    "\n",
    "    # Create a dictionary suitable for Dataset.from_dict\n",
    "    data_dict = {\n",
    "        'image': images,\n",
    "        'image_id': image_ids,\n",
    "        'caption': captions\n",
    "    }\n",
    "\n",
    "    # Create a Dataset from the dictionary\n",
    "    dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = convert_to_datasetdict(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = convert_to_datasetdict(test_datasetst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dict = convert_to_datasetdict(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dict,\n",
    "    \"test\": test_dict,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"mdwiratathya/ROCO-radiology\", commit_message=\"first commit\", commit_description=\"adding train/validation/test split, with image stored as PIL object\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
